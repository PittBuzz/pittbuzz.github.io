## Setup

### Load packages

```{r load-packages, message = FALSE}
library(ggplot2)
library(dplyr)
library(statsr)
library(BAS)
library(GGally)       
library(grid)          
library(gridExtra)   
library(MASS)         
library(rstanarm)

```

### Load data

Make sure your data and R Markdown files are in the same directory. When loaded
your data file will be called `movies`. Delete this note when before you submit 
your work. 

```{r load-data}
load("movies.Rdata")

dim(movies)
glimpse(movies)
```



* * *

## Part 1: Data
The data set is comprised of 651 randomly sampled movies produced and released before 2016.

* * *

## Part 2: Data manipulation

Create new variables based on existing fields to categorize potential variates
```{r mutate columns}
movies2 <- movies %>%
  mutate(feature_film = as.factor(ifelse(title_type == 'Feature Film', 'yes', 'no'))) %>%
  mutate(drama = as.factor(ifelse(genre == 'Drama', 'yes', 'no'))) %>%
  mutate(mpaa_rating_R = as.factor(ifelse(mpaa_rating == 'R', 'yes', 'no'))) %>%
  mutate(oscar_season = as.factor(ifelse(thtr_rel_month %in% c(10,11,12) , 'yes', 'no'))) %>%
  mutate(summer_season = as.factor(ifelse(thtr_rel_month %in% c(5,6,7,8) , 'yes', 'no'))) 

```

```{r review new df}
head(movies2)
glimpse(movies2)
dim(movies2)
```


* * *

## Part 3: Exploratory data analysis

The response variable of interest is <b> audience score</b>. To analyse the newly created features, we will create a subset of the full dataset,  which only contains the required data. Then will remove the NA's from the dataset and check the audience_score data via a density function 
(incl. histogram) of the audience_score. I also applied the mean, median and the mode values to the figure. The dmode function is part of the  utilities script and is available in the appendix.  

```{r}
features <- c('audience_score', 'oscar_season', 'summer_season', 'mpaa_rating_R', 'drama', 'feature_film' )
```

```{r}
dmode <- function(x) {
  den <- density(x, kernel = c("gaussian"))
  ( den$x[den$y == max(den$y)] )   
}  
```

```{r}
  # Create a new dataset to explicitely show dataset containing NA's
  data.analysis <- movies2[, features]
  
  # remove NA's
  data.analysis <- data.analysis[complete.cases(data.analysis), ]
  # calculate the mode of the distribution
  mode <- dmode(data.analysis$audience_score)
```

```{r}
  ggplot(data = data.analysis, aes(x = audience_score, y = ..density..)) +
  geom_histogram(bins = 40, fill = 'steelblue', colour = 'black') + #bdbdbd
  geom_density(size = 1, colour = 'brown') + #cccccc
  geom_vline(data = data, mapping = aes( xintercept = mean(data$audience_score),
             colour = 'steelblue', show_guide = F ), size = 1.5) +
  geom_vline(data = data,mapping = aes( xintercept = median(data$audience_score),
             colour = 'green', show_guide = F), size = 1.5) +
  geom_vline(data = data, mapping = aes( xintercept = mode, colour = 'red', show_guide = F), 
             size = 1.5) +
  geom_text(data = data, aes( x = (mean(data$audience_score) - 5), y = .020, label = 'mean',
            colour = 'steelblue'), size = 4, parse = T) +
  geom_text(data = data,aes( x = (median(data$audience_score) + 5),y = .020,  label = 'median',
            colour = 'green'), size = 4, parse = T) +
  geom_text(data = data, aes( x = (mode + 5), y = .020, label = 'mode', colour = 'red'),
            size = 4, parse = T)
```

```{r}
  mean(data.analysis$audience_score)
  median(data.analysis$audience_score)
  mode
  
  summary(data.analysis$audience_score)
  IQR(data.analysis$audience_score)
```

The audience_score shows a slight left skewed structure. The IQR of the audience_score is 34 (1st Qu - 46 and 3rd Qu. 80), while the mean is around 62.4, the median is 65 
and the mode is about 80.42


<h4><b>Analysis of the newly created features</b></h4>
We'll have a look at the newly created features <b>oscar_season, summer_season, mpaa_rating_R, drama and feature_film</b>. We create a boxplot for each feature, 
comparing them with the audience_score. We will also analyse the variability of the new features by comparing them to each other.

```{r}
data.grouped <- gather(data.analysis, 'features', 'flag', 2:6)
p1 <- ggplot(data = data.analysis, aes(x = summer_season, y = audience_score, fill = summer_season)) + 
      geom_boxplot() + ggtitle('Audience score vs summer season') + xlab('summer season') + 
      ylab('Audience Score') + scale_fill_brewer(name = "summer season")
p2 <- ggplot(data = data.analysis, aes(x = oscar_season, y = audience_score, fill = oscar_season)) + 
      geom_boxplot() + ggtitle('Audience score vs oscar_season') + xlab('oscar_season') + 
      ylab('Audience Score') + scale_fill_brewer(name = "oscar_season")
p3 <- ggplot(data = data.analysis, aes(x = drama, y = audience_score, fill = drama)) + geom_boxplot() +
      ggtitle('Audience score vs drama') + xlab('drama') + ylab('Audience Score') + 
      scale_fill_brewer(name = "drama")
p4 <- ggplot(data = data.analysis, aes(x = feature_film, y = audience_score, fill = feature_film)) + 
      geom_boxplot() + ggtitle('Audience score vs feature_film') + xlab('feature_film') + 
      ylab('Audience Score') + scale_fill_brewer(name = "feature_film")
p5 <- ggplot(data = data.analysis, aes(x = mpaa_rating_R, y = audience_score, fill = mpaa_rating_R)) + 
      geom_boxplot() + ggtitle('Audience score vs mpaa_rating_R') + xlab('mpaa_rating_R') + 
      ylab('Audience Score') + scale_fill_brewer(name = "mpaa_rating_R")
# arrange the previously created plots 
grid.arrange(p1, p2, p3, p4, p5, ncol = 2)
```
To compare the variability of the different features we aggregate all the above shown boxplots in one graph. We also prep a summary 
of all the newly created features to closer compare them.

```{r}
      ggplot(data = data.grouped, aes(x = features, y = audience_score, fill = flag)) + geom_boxplot() +
      ggtitle('Audience score vs grouped featues') + xlab('grouped featues') + ylab('Audience Score') +
      theme(axis.text.x = element_text(angle = 60, hjust = 1)) + 
      scale_fill_brewer(name = "grouped featues")
```

```{r}
data.grouped %>%
 group_by(features, flag) %>%
 summarise(mean = mean(audience_score), median = median(audience_score), min = min(audience_score), 
           max = max(audience_score), IQR = IQR(audience_score))

```
The new features do not show much variability in the different features, which <b>leads to the conclusion that none of the above 
features are valuable towards the prediction of the audience score. The only feature which might be relevant, could be the 
'feature_film' feature. There seems to be a clear differentiation.</b> But we shouldn't jump into conclusions right away, 
therefore we will continue analysing the data.

Finally we will have a look at the other features used in the dataset. I left out the features as runtime, imdb_rating, imdb_num_votes, 
Above a selection of Density plots have been visualized. The plots show different features compared to the audience score.

```{r}

features <- c( 'audience_score', 'feature_film', 'drama', 'runtime', 'mpaa_rating_R', 'thtr_rel_year', 
               'oscar_season', 'summer_season', 'imdb_rating', 'imdb_num_votes', 'critics_score', 
               'best_pic_nom', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win', 
               'top200_box')


    # Create a new dataset to explicitely show dataset containing NA's
    data.model <- data[, features]
    
    # remove NA's
    data.model<- data.model[complete.cases(data.model), ]
    p1 <- ggplot(data.model, aes(audience_score, fill = feature_film))
    p1 <- p1 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. feature_film") + 
               labs(x = "feature film", y = "Density")
    
    p2 <- ggplot(data.model, aes(audience_score, fill = drama))
    p2 <- p2 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. drama") + 
               labs(x = "drama", y = "Density")
    
    p3 <- ggplot(data.model, aes(audience_score, fill = top200_box))
    p3 <- p3 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. top200_box") +
               labs(x = "top200 box", y = "Density")
    
    p4 <- ggplot(data.model, aes(audience_score, fill = oscar_season))
    p4 <- p4 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. oscar_season") + 
               labs(x = "oscar season", y = "Density")
    p5 <- ggplot(data.model, aes(audience_score, fill = summer_season))
    p5 <- p5 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. summer_season") + 
               labs(x = "summer season", y = "Density")
    
    p6 <- ggplot(data.model, aes(audience_score, fill = best_pic_nom))
    p6  <- p6 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_pic_nom") + 
                labs(x = "best pic nom", y = "Density")
    
    p7 <- ggplot(data.model, aes(audience_score, fill = best_pic_win))
    p7 <- p7 + geom_density(size=1, colour="darkgreen") + labs(title = "Dist. of audience score vs. best pic win") + 
               labs(x = "best pic win", y = "Density")
    
    p8 <- ggplot(data.model, aes(audience_score, fill = best_actor_win))
    p8 <- p8 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_actor_win") + 
               labs(x = "best actor win", y = "Density")
    
    p9 <- ggplot(data.model, aes(audience_score, fill = best_dir_win))
    p9 <- p9 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_dir_win") + 
               labs(x = "best dir win", y = "Density")
    
    p10 <- ggplot(data.model, aes(audience_score, fill = best_actress_win))
    p10 <- p10 + geom_density (alpha = 0.2) + labs(title = "Dist. of audience score vs. best_actress_win") + 
                 labs(x = "best actress win", y = "Density")
    
    grid.arrange(p1, p2, p3, p4, p5, p6, p7, p8, p9, p10, ncol = 2)
```

<h4><b>Hypothesis Test</b></h4>

As with the frequentist approach we use these data to perform basic inference on μ the audience_score of all movies in the dataset. To do this we will use the bayes_inference function, which will allow us to construct credible intervals perform a hypothesis test and calculate Bayes factors for a variety of different circumstances.
The main goal is to investigate if the newly created features influence the audience_score. Remember we assumed that the new fetures will not influence the audience_score very much. 

We will conduct a Bayesian hypothesis test by calculating a Bayes factor for each feature. The following hypothesis is meant to be generic and needs to be adjusted for each individual feature 
(feature_film, drama, mpaa_rating_R, oscar_season and summer_season). We assigned a uniform prior and use a 'two sided' direction for each analysis. I'll just print on of the summaries to indicate the information which is contained. 
All others will be suppressed to save some space. Enable the params - show_res and show_summ by setting the value to TRUE.

$$ H_1: \mu = (value) $$

$$ H_2: \mu \ne (value) $$
```{r}
bayes_inference(y = audience_score, data = data.analysis, cred_level = 0.95, statistic = "mean", type = "ci", null = 0, show_res = FALSE, show_summ = FALSE)
bayes_inference(x = feature_film, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = TRUE, show_summ = TRUE)
bayes_inference(x = feature_film, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE )
bayes_inference(x = oscar_season, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = summer_season, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = drama, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)
bayes_inference(x = mpaa_rating_R, y = audience_score, data = data.analysis, cred_level = 0.95, hypothesis_prior = c(0.5, 0.5), statistic = "mean", type = "ht", null = 0, alternative = 'twosided', show_res = FALSE, show_summ = FALSE)

```
b>Bayes Factors</b>

feature           | BF[H1:H2] | BF[H2:H1] | Evidence against    |                    
------------------|-----------|-----------|---------------------|
feature_film      |           | 4.22e+13  |   H1 (Very Strong)  |
oscar_season      | 13.3993   |           |   H2 (Positive)     |
summer_season     | 19.8084   |           |   H2 (Positive)     |
drama             |           | 22.6567   |   H1 (Positive)     |
mpaa_rating_R     | 23.9679   |           |   H2 (Positive)     |

The newly created features seem not to be very indicative for the prediction of the audience_score as shown in the table above. The only feature which shows a differentiation is 'feature_film'. There is strong evidence against H1, which means that there is a significant difference in mean audience_score for feature- and non-feature films. While the data 
provides a positive evidence that there is no difference between 'audience_scores' and all the other (new) features (mpaa_rating_R, oscar_season, summer_season, drama).


* * *

## Part 4: Modeling

The best model is not always the most complicated. Sometimes including variables that are not evidently important, can actually reduce the accuracy of predictions. 
In practice, the model that includes all available explanatory variables is often referred to as the full model. The full model may not be the best model, and if 
it isn't, we want to identify a smaller model that is preferable.

<b>Full model:</b><br>
<b>audience_score ~ feature_film + drama + runtime + mpaa_rating_R + thtr_rel_year + oscar_season + summer_season + imdb_rating + imdb_num_votes + critics_score + 
best_pic_nom + best_pic_win + best_actor_win + best_actress_win + best_dir_win + top200_box</b>

<b>Bayesian Model Averaging (BMA)</b></br>
A comprehensive approach to address model uncertainty is Bayesian model averaging, which allows us to assess the robustness of results to alternative specifications by 
calculating posterior distributions over coefficients and models. Given the 17 features (n) there can be 2^n = 2^17 possible models. We will explore model uncertainty using 
posterior probabilities of models based on BIC. We will use BIC as a way to approximate the log of the marginal likelihood. The Bayesian information criterion (BIC) runs through </br></br>
several fitted model objects for which a log-likelihood value can be obtained, according to the formula -2*log-likelihood + npar*log(nobs), where npar represents the number of 
parameters and nobs the number of observations in the fitted model. 

We show the model selection based on two different priors (Zellner and BIC), but we will also compare the model selection based on the following priors
"g-prior", "AIC", "hyper-g-n", "EB-local". The last part will indicate the Akaike information criterion (AIC), which uses a backward feature elimination method 
and the model which is proposed by this approach.

```{r}
features <- c( 'audience_score', 'feature_film', 'drama', 'runtime', 'mpaa_rating_R', 'thtr_rel_year', 
               'oscar_season', 'summer_season', 'imdb_rating', 'imdb_num_votes', 'critics_score', 
               'best_pic_nom', 'best_pic_win', 'best_actor_win', 'best_actress_win', 'best_dir_win', 
               'top200_box')
    # Create a new dataset to explicitely show dataset containing NA's
    data.model <- data[, features]
    
    # remove NA's
    data.model<- data.model[complete.cases(data.model), ]
    # str(data.model)
    formula <- getFormula(data.model, 'audience_score')
```
<h4><b>Bayesian Information Criterion</b></h4>
```{r}
    audience.BIC = bas.lm(
      formula = formula,
      prior = "BIC",
      modelprior = uniform(),
      data = data.model
    )
```
The object provides the marginal posterior inclusion probabilities. The marginal posterior inclusion probability is the probability that the predictor variable is included in the model. We can also extract this directly from the object as
```{r}
    audience.BIC$probne0
```
<b>Coefficient Summaries</b><br>
The summary outputs have been aggregated for convenience purposes. The section shows the marginal posterior mean, standard deviation and posterior inclusion probabilities obtained by BMA.
```{r}
audience.BIC.coef = coef(audience.BIC)
interval   <- confint(audience.BIC.coef)
names <- c("post mean", "post sd", colnames(interval))
interval   <- cbind(audience.BIC.coef$postmean, audience.BIC.coef$postsd, interval)
colnames(interval) <- names
interval
```
The summary of the given model indicates the top 5 models. The model containing the intercept, runtime, mdb_rating and critics_score, show the best performance, but as one can see, the top 2 models contain only about 27 percent of the posterior mass over all models. 
```{r}
summary(audience.BIC)
```
<b>Model Space</b><br>
To visualize the space of models (by default the top 20 models in terms of their posterior probabilities), we may use the image function. The image below shows the best model 
on the left hand side (indicated with a 1). It indicates that the best model can be built by using the features - runtime, critics_score and imdb_rating.
```{r}
image(audience.BIC, rotate = FALSE)
```

```{r}
par(mfrow=c(2,2))
plot(audience.BIC, ask = F, add.smooth = F, caption="", col.in = 'steelblue', col.ex  = 'darkgrey', pch=17, lwd=2)
```
The first is a plot of residuals and fitted values under Bayesian Model Averaging. Ideally, of our model assumptions hold, we will not see outliers or non-constant variance. The second plot shows the cumulative probability of the models in the order that they are sampled. This plot indicates that the cumulative probability is levelling off as each additional model adds only a small increment to the cumulative probability, which earlier, there are larger jumps corresponding to sampling high probability models. The third plot shows the dimension of each model (the number of regression coefficients including the intercept) versus the log of the marginal likelihood of the model. The last plot shows the marginal posterior inclusion probabilities (pip) for each of the covariates, with marginal pips greater than 0.5 shown in red. The variables with pip > 0.5 correspond to what is known as the median probability model. Variables with high inclusion probabilities are generally important for explaining the data or prediction, but marginal inclusion probabilities may be small if there are extreme correlations among predictors, like p-values may be large in the presence of multicollinearity.

<h4><b>Zellner-Siow Cauchy</b></h4>

We are going to use the Zellner-Siow Cauchy prior with an MCMC method. We define 10^6 iterations for the model.
```{r}
audience.ZS =  bas.lm(formula = formula, 
                   data = data.model,
                   prior="ZS-null",
                   modelprior=uniform(),
                   method = "MCMC", 
                   MCMC.iterations = 10^6) 
```
<h4><b>Analysis with different priors </b></h4>
To indicate which parameters would be favoured output would be quiet exhaustive. So we only used the reduced feature set proposed by Zellner-Siow and BIC.
```{r}
    features <-
      c( 'audience_score', 'runtime', 'imdb_rating', 'critics_score')
    # Create a new dataset to explicitely show dataset containing NA's
    data.final <- data[, features]
    
    # remove NA's
    data.final<- data.final[complete.cases(data.final), ]
    formula <- getFormula(data.final, 'audience_score')
      n = nrow(data.final)   
      movie.g   = bas.lm(formula, data = data.final, prior = "g-prior",   a=n, modelprior = uniform()) 
      movie.ZS  = bas.lm(formula, data = data.final, prior = "ZS-null",   a=n, modelprior = uniform())
      movie.BIC = bas.lm(formula, data = data.final, prior = "BIC",       a=n, modelprior = uniform())
      movie.AIC = bas.lm(formula, data = data.final, prior = "AIC",       a=n, modelprior = uniform())
      movie.HG  = bas.lm(formula, data = data.final, prior = "hyper-g-n", a=3, modelprior = uniform()) 
      movie.EB  = bas.lm(formula, data = data.final, prior = "EB-local",  a=n, modelprior = uniform())
      
      probne0 = cbind(movie.BIC$probne0, movie.g$probne0, movie.ZS$probne0, movie.HG$probne0, movie.EB$probne0, 
                      movie.AIC$probne0)
      colnames(probne0) = c("BIC","g", "ZS", "HG", "EB", "AIC")
      rownames(probne0) = c(movie.BIC$namesx)
      
      for (i in 2:4) {
        barplot(height = probne0[i,], ylim = c(0,1), main = movie.g$namesx[i], col.main = 'darkgrey', 
                col = 'steelblue')
```
All models favour the reduced model with only the 'imdb_rating' and 'critics_score' except AIC. AIC would also include the 'runtime' feature.

<h4><b>Final Model</b></h4>
Based on the previous analysis we will use the following model to continue with the prediction: <b>audience_score ~ imdb_rating + critics_score</b></br>
It seems a little fuzzy to me, that a final model basically follows 'one' predictor - 'critics_score'. I suppose there must be better predictors, 
which we weren't asked to use.
```{r}
    features <- c( 'audience_score', 'imdb_rating', 'critics_score' )
    # Create a new dataset to explicitely show dataset containing NA's
    data.final <- data[, features]
    
    # remove NA's
    data.final<- data.final[complete.cases(data.final), ]
    formula <- getFormula(data.final, 'audience_score')
    
    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = data.final,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    
```
<h4><b>Inference with model selection</b></h4>
In addition to using BMA, we can use the posterior means under model selection. This corresponds to a decision rule that combines estimation and selection. 

<b>Bayesian model averaging</b>
```{r}
model <- audience.ZS
BMA  = predict(model, estimator="BMA")
```

```{r}
HPM = predict(model, estimator="HPM")
model.ZS = coef(model)
model.ZS$conditionalmeans[HPM$best,]
model.ZS$conditionalsd[HPM$best,]
```

```{r}
BMA <- predict(model, estimator="BMA")  # Bayesian model averaging, using optionally only the 'top' models 
HPM <- predict(model, estimator="HPM")  # The highest probability model 
BPM <- predict(model, estimator="BPM")  # The model that is closest to BMA predictions under squared error loss. 
# MPM <- predict(model, estimator="MPM")  # The median probability model of Barbieri and Berger
library(GGally)
ggpairs(data.frame(HPM = as.vector(HPM$fit),  #this used predict so we need to extract fitted values
                   BPM = as.vector(BPM$fit),  # this used fitted
                   BMA = as.vector(BMA$fit))) # this used predict
BPM = predict(audience.ZS, estimator="BPM", se.fit=TRUE)
audience.conf.fit = confint(BPM, parm="mean")
audience.conf.pred = confint(BPM, parm="pred")
# cbind(audience.conf.fit, audience.conf.pred)
# plot(audience.conf.fit)
```





* * *

## Part 5: Prediction

Finally, I'll provide the predicted value, based on a 95% CI. We used the movie <b>Sully</b> for that purpose. We couldn't find the actual audience score, 
therefore it is unclear, how precise the predicted values is, but we went the route to train the model with a training set and used the test set to validate the 
actual values.
```{r}
    features <- c( 'audience_score', 'imdb_rating', 'critics_score' )
    # Create a new dataset to explicitely show dataset containing NA's
    data.final <- data[, features]
    
    # remove NA's
    data.final<- data.final[complete.cases(data.final), ]
    
    ds <- splitTrainTestSet(data = data.final, 0.6)
    df.train <- data.frame(ds$train)
    df.test  <- data.frame(ds$test)   
    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = df.train,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    
    predict.value <- ds$test[2,]
    pred.A <- predict(audience.ZS, newdata = predict.value, estimator="BMA")
    # Error in percent  
    Error_in_percent.A = round(100 - (round(pred.A$Ybma, 2) * 100 / predict.value$audience_score), 2)
    predict.value <- ds$test[34,]
    pred.B <- predict(audience.ZS, newdata = predict.value, estimator="BMA")
    # Error in percent  
    Error_in_percent.B = round(100 - (round(pred.B$Ybma, 2) * 100 / predict.value$audience_score), 2)
    
    # ===============================================================================================
    # Sully 
    # ===============================================================================================
   
    audience.ZS =  bas.lm(audience_score ~ .,  
                          data = data.final,
                          prior = "ZS-null",
                          modelprior = uniform(),
                          method = "MCMC", 
                          MCMC.iterations = 10^6)    
    
    predict.value <- data.frame( imdb_rating = 7.6, critics_score = 74, audience_score = 80 )
    predict.sully <- predict(audience.ZS, newdata = predict.value, estimator="BMA")
    predict.sully$Ybma
```
To verify the model we initially separate the dataset into a training and a test dataset. We train the model with the training data and afterwards use some test sets to verify the defined model. 

The predicted value for Sully is ~80, which might be a realistic value, based on the previously validated values. The actual audience score for this particular movie has a lower bound of approximately 60.42 and a higher bound of approximately 100.03.

* * *

## Part 6: Conclusion
The predictive model presented here is used to predict the audience scores for a movie. Using Bayesian model average many models can be constructed to perform 
better predictions. The proposed linear model shows a 'fairly good' prediction rate, but it should be noted that the model is based on a very small sample. 
Fact is that imdb_rating has the highest posterior probability, and that basically all of the newly created features were useless to support a better prediction. Creating a model, which has a high 
predictive power is not so easy to reach. Using Bayes for better prediction is only one part of the game. It might be beneficial to gather more data or try to extend the feature engineering part,
which means to creating new meaningful features from existing or gather data for new features. I doubt that a feature as 'summer_season' or if a movie has an mpaa_rating - R is supporting the prediction.
 
I have my personal view on how I would rely on other features to evaluate a audience score (e.g. genre, actors etc.). What I figured as well is, that Lower bound audience_scores show a much higher difference. 
Therefore we might think about a second or higher order polynomial model. But we still need be carefull to find a decent balance between over-, and underfitting the model. 

One last word. Having selected the model based on a bayesian model does not per se mean that the model automatically generalizes better. So you are still free to add or remove certain predictors for the 
greater good. It's called the expert opinion.
