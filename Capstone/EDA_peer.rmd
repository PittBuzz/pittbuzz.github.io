---
title: "Peer Assessment I"
output:
  html_document: 
    pandoc_args: [
      "--number-sections",
    ]
---


First, let us load the data and necessary packages:

```{r load, message = FALSE}
load("ames_train.Rdata")
library(MASS)
library(dplyr)
library(ggplot2)
```

#
Make a labeled histogram (with 30 bins) of the ages of the houses in the data set, and describe the distribution.


```{r Q1}
ames_train <- ames_train %>% 
        mutate(Age = 2021 - Year.Built)

ggplot(ames_train, aes(x = Age), labs(title = "Housing Age Distribution"))  + 
  geom_histogram(bins = 30)

summary(ames_train$Age)

```


* * *

The mean age of homes(48.8) is larger than the median age of 46.0.
This positively skewed (Right-skewed) distribution of the Age feature is confirmed by the histogram. 

Most houses have been built within the last 20 years with peak construction in the last 10.

A second peak indicates a boom in the 1960s.


* * *


#
The mantra in real estate is "Location, Location, Location!" Make a graphical display that relates a home price to its neighborhood in Ames, Iowa. Which summary statistics are most appropriate to use for determining the most expensive, least expensive, and most heterogeneous (having the most variation in housing price) neighborhoods? Report which neighborhoods these are based on the summary statistics of your choice. Report the value of your chosen summary statistics for these neighborhoods.


```{r Q2}
ames_train %>%
  select(price, Neighborhood) %>%
  group_by(Neighborhood) %>%
  summarize(median = median(price), max = max(price), min = min(price), sd = sd(price)) %>%
  arrange(desc(median))

ggplot(ames_train, aes(y = price, x = reorder(Neighborhood, price, FUN = median), fill = Neighborhood)) + 
  geom_boxplot() + 
  coord_flip()
       


```


* * *

Box plots best display the price variations of neighborhoods. I think it is easier to make observations by reversing the axes and fill by neighborhoods. 

As the  housing prices are positively skewed,  median prices are the better estimate than the mean for making generalizations. 

The  StoneBr and NridgHt neighborhoods have both the most expensive houses and the greatest price ranges.
The least expensive neighborhoods are the MeadowV, BrDale and the Old Town (which has the lowest housing prices in the dataset).



* * *

# 

Which variable has the largest number of missing values? Explain why it makes sense that there are so many missing values for this variable.

```{r Q3}
apply(ames_train, 2, function(x) sum(is.na(x))) %>% sort %>%
  tail()

```


* * *

Pool.QC has the largest "NA" entries as "No Pool" is the NA value in the data and most homes do not have pools. 


* * *

#

We want to predict the natural log of the home prices. Candidate explanatory variables are lot size in square feet (Lot.Area), slope of property (Land.Slope), original construction date (Year.Built), remodel date (Year.Remod.Add), and the number of bedrooms above grade (Bedroom.AbvGr). Pick a model selection or model averaging method covered in the Specialization, and describe how this method works. Then, use this method to find the best multiple regression model for predicting the natural log of the home prices.


```{r Q4}
library(BAS)

log_lm = lm(log(price) ~ Lot.Area 
            + Land.Slope 
            + Year.Built 
            + Year.Remod.Add
            +Bedroom.AbvGr,
            data = ames_train)

summary(log_lm)

log_bic = bas.lm(log(price)~Lot.Area
                 + Land.Slope
                 + Year.Built
                 + Year.Remod.Add
                 +Bedroom.AbvGr, 
                 data = ames_train,
        prior = "BIC",
        modelprior = uniform(),
        method = "MCMC" )

summary(log_bic)

```

* * *

The best model is evaluated using Bayesian Model Averaging (BMA). 

BIC is selected as a prior distribution because it provides the highest Posterior Probabilities values. 
The model recommends to keep all of the selected variables as the best fitted predictors of the home price through BMA (BIC and MCMC). The linear model seems to infer concurrence with BMA.


* * *

#

Which home has the largest squared residual in the previous analysis (Question 4)? Looking at all the variables in the data set, can you explain why this home stands out from the rest (what factors contribute to the high squared residual and why are those factors relevant)?


```{r Q5}
plot(log_lm)

House <- ames_train[which(log_lm$residuals^2 == max((log_lm$residuals)^2)),]

House

predPrice <- exp(log_lm$fitted.values[which(log_lm$residuals^2 == max((log_lm$residuals)^2))])

predPrice

```

* * *

 Lot PID number 902207130 is the outlier with highest squared residual when running plot residuals vs fitted. 
 
The house was sold for ~ $12,000 in 1928. The predicted price is ~ $103176 which makes it a significant outlier.

* * *

#

Use the same model selection method you chose in Question 4 to again find the best multiple regression model to predict the natural log of home prices, but this time **replacing Lot.Area with log(Lot.Area)**. Do you arrive at a model including the same set of predictors?


```{r Q6}
log_bic = bas.lm(log(price) ~ log(Lot.Area)
                 + Land.Slope
                 + Year.Built
                 + Year.Remod.Add
                 + Bedroom.AbvGr, 
                 data = ames_train,
        prior = "BIC",
        modelprior = uniform(),
        method = "MCMC")

summary(log_bic)

```

The new model now recommends to remove both the Land.Slope variables.

#



* * *

#

Do you think it is better to log transform Lot.Area, in terms of assumptions for linear regression? Make graphs of the predicted values of log home price versus the true values of log home price for the regression models selected for Lot.Area and log(Lot.Area). Referencing these two plots, provide a written support that includes a quantitative justification for your answer in the first part of question 7.

```{r Q7}
lm1 <- lm(log(price) ~ Lot.Area
          + Land.Slope
          + Year.Built
          + Year.Remod.Add
          + Bedroom.AbvGr, 
          data = ames_train)

lm2 <- lm(log(price) ~ log(Lot.Area)
          + Land.Slope
          + Year.Built
          + Year.Remod.Add
          + Bedroom.AbvGr, 
          data = ames_train)

summary(lm1)
summary(lm2)

plot(lm1)
plot(lm2)

Price <- log(ames_train$price)

pred_lm1 <- predict.lm(lm1, ames_train)
pred_lm2 <- predict.lm(lm2, ames_train)

plotdf <- data.frame(log_Price = Price, PredictedPrice = c(pred_lm1, pred_lm2), predictionType=c(rep(c("Lot.Area"), 1000), rep(c("log(Lot.Area))"), 1000)), difference = c((Price - pred_lm1), (Price - pred_lm2)))

ggplot(data = plotdf, aes(x = log_Price, y = PredictedPrice, color = predictionType)) +
    geom_point(, alpha = 0.5) +
    geom_abline(slope = 1, intercept = 0) + 
    labs(title = "Predicted log(Price) vs Actual log(Price) for Both Models", y = "Predicted log(Price)", x = "Actual log(Price)")

```

* * *



The Adjusted R squared values  do show that log transformation does improve modelling as the value of R squared increases from 0.56 to 0.60.

Predicted prices versus actual prices for each model are included in the same plot. A 'y = x' corresponding to predictions for all values of price is also added to provide a reference. The plot shows that the results from both models are quite similar.

 


* * *
###